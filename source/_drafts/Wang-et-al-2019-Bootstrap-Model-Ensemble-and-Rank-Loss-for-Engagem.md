---
title: Wang et al. - 2019 - Bootstrap Model Ensemble and Rank Loss for Engagem
categories: 
  - 科研学习 - Engagement
tags:
  - Engagement
katex: true:
---

# 摘要

本文主要采用 bootstrap 策略和两个新的损失函数来研究头部特征和身体特征。我们使用了长短时记忆（LSTM）网络的多实例学习框架，并做出了三点贡献。
- 首先，除了凝视和头部姿势特征外，探索了面部标志性特征。
- 第二，设计了一个秩损失作为正则化。
- 第三，采用经典的 bootstrap 聚合方法进行模型集成，对某个训练数据进行多次随机采样，然后对模型预测进行平均。

# 简介

在常规课程中，教师可以直接观察学生的行为并获得反馈。然而，网络课程的远程性和庞大性要求采用新的方法来提供反馈并指导教师干预教学活动。

专注度分为四个等级（0，0.33，0.66，1），分别表示完全脱离、勉强专注、专注和高度专注[12]。

我们提出了一个具有三种不同数据分割的 bootstrap 集成模型。具体地说，我们对某个训练数据进行多次随机抽样，然后将模型预测平均值作为我们的 bootstrap 方法。受参与强度可以按值排序这一事实的启发，我们设计了一种秩损失正则化方法，该正则化方法在距离对和相邻对的特征之间施加一个 margin。

框架由四部分组成，即特征提取、回归网络（LSTM+FC）、特征融合和模型集成。我们提取了三种类型的特征，即 C3D、Openface 和 Openpose（有70个人脸标记）。然后将特征输入回归网络，预测专注度。最后，特征融合和模型集成对每个单回归模型进行增强。

# 相关工作

学生参与度是影响学习的一个重要因素[28]，但在有效的参与度预测方面仍存在许多挑战。以往许多关于自动敬业度识别的研究都集中在感知敬业度（即外部人员接受的敬业度）上[9]。已经提出了各种自动参与预测系统，使用多模态信息，如学生反应[13]、面部[10、14、16、20、22、23]或学习视频中的身体运动[5、26]、测验中的行为[11]，甚至高级生理学和神经学测量[8]。其中，视频数据是捕获方便性和粒度之间的一个很好的折衷。使用视频，怀特希尔等人。
[25]分析面部特征并构建用于预测交战的SVM分类器。除了受试者的面部表情外，他们的姿势在参与中也起着重要的作用，这在[2]中得到了应用。另一项有趣的工作是利用面部特征和测试日志来分析他们的学习水平[4]。在EmotiW2018中，一些基于多实例学习的方法[18,27]取得了显著的改进，这促使我们设计了一个多实例学习框架，用于参与度回归。

# 方法

## 系统管道

![系统框架](/images/Wang-et-al-2019-Bootstrap-Model-Ensemble-and-Rank-Loss-for-Engagem/2021-01-06-19-11-32.png)

首先，我们将视频分割成若干段，提取各种特征，如卷积三维（C3D）、Openface 和 OpenPose 特征。

其次，我们将各种特性发送到回归网络（LSTM + FC）。回归网络输出各段的专注度回归值。

然后，融合特征用于聚合（平均分段的输出）。

最后，假设不同的模式会产生不同的参与度值。通过秩损失（RL）和均方误差损失对回归网络进行优化。

此外，受 bootstrap 方法的启发，我们对数据集进行三次重采样，作为经验分布来估计数据集的真实分布。

## 多实例学习框架

以往的研究大多是直接从整个视频中提取多模态特征，这往往会导致片段细节信息的丢失和冗余。考虑到上述缺点，我们将学生参与度描述为一个多实例回归。视频序列被划分为 $k$ 个片段，即 $V=[s_1，s_2，s_3，\dots，s_k]$ ，其中 $s_i$ 表示第 $i$ 个视频片段，并且每个视频片段与整个视频具有相同标签。从一个片段中获得 $M$ 个不同的模态特征 $F_k=[f^1_k，f^2_k，f^3_k，\dots，f^m_k]$，并将它们输入到我们的框架中。这些特征被输入到一个 LSTM 和三个全连接的层中。

注意，不同特征模态的回归参数是不同的，而相同模态的回归参数可以采用共享权重。然后可以得到每段 $s_i$ 的回归值 $r_i$。将所有片段的平均参与度作为视频的最终预测参与度，并通过标注参与度和回归结果之间的 RL 和 MSE 损失来优化网络。

训练和验证视频数据以 30fps 的速度采集，每秒提取 5 帧，分辨率为 640×480 。由于视频长度不同，我们通过平均长度将视频分成若干段。一个视频序列的视频片段作为输入。然后使用不同的工具或预训练模型提取多模态特征。

## 多模态特征

在一段视频中，受试者的动作提现其注意力集中程度。面部和姿势的变化反映了运动幅度和参与度。此外，写作、思考或凝视等具体动作也可以帮助我们预测参与度。在这些因素和直觉的激励下，我们从两个角度提取特征，其中包括三种模式（OpenFace、OpenPose和C3D特征）

**OpenFace and OpenPose 特征**：注视、头部和身体位置。参与度与人体运动的程度呈负相关。如果学生注意力集中在学习内容上，那么他的注视和姿势的动作可能很少。如果受试者参与度很低，则应该心不在焉，目光飘忽不定，身体动作幅度较大。使用 OpenFace[1]捕捉凝视和头部运动特征，同时通过 OpenPose[19] 捕捉身体姿势特征。

**眼睛注视**：OpenFace 提供眼睛注视估计和跟踪数据，并使用它获得每个人脸的注视坐标。然后，计算每个片段中的点与平均位置的平均方差，得到一个片段中 6 个与注视相关的特征。

**头部姿势**：OpenFace 能够提取头部姿势（平移和方向）信息以及面部标记检测。头部的运动有时与视线漂移相一致，但有时不是。因此，头部姿势特征应该与注视互补，同时使用头部姿势和眼睛注视丰富特征。

**身体姿势**：人体运动也是参与度的一个指标。身体运动所反映的行为应该包含更多特定目的形成，如沉思或书写笔记。通过 OpenPose 检测身体关键点可以捕捉到这些动作。我们选取了 14 个频繁检测到的上半身运动的关键点，并用它们的标准差作为特征。由于 OpenFace 和 OpenPose 的工作方式不同，使得人体和人脸的信息不能同步。我们认为身体特征是一种独立的特征。

虽然 OpenFace 和 OpenPose 特征包括面部、头部和身体，但它是相当有限的。这些特征只能代表不同成分的运动程度，而忽略了具体的动作和注视变化模式。更严重的是，使用开放库的高级提取会丢失很多空间特征，尤其是视频中的人脸。因此，需要提取动作特征来丰富动作特征。

**动作特征**：C3D[15，21]是一种新的深空时特征。由于我们已经对面部特征的几种特征进行了建模，C3D可以在时空域中作为身体动作的一种鲁棒表示。

具体地说，使用 OpenPose 提供的 body landamarks 裁剪身体，然后使用 C3D 预训练模型（在 Sports-1M 数据集中）在每个片段中提取 C3D 特征。图像的大小调整为 228×228，用于 C3D 输入。最终得到 768 维的 C3D 特征作为一个片段的模态。

